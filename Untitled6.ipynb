{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KKiosLlKq-a_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f26d805-014b-4170-ba91-a2c5c58f9f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-12 14:29:57--  https://raw.githubusercontent.com/githubgurus/Health_Data/main/harvard_health_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16964592 (16M) [text/plain]\n",
            "Saving to: ‘harvard_health_data.txt’\n",
            "\n",
            "harvard_health_data 100%[===================>]  16.18M   102MB/s    in 0.2s    \n",
            "\n",
            "2023-10-12 14:29:57 (102 MB/s) - ‘harvard_health_data.txt’ saved [16964592/16964592]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/githubgurus/Health_Data/main/harvard_health_data.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/githubgurus/Health_Data/main/mayo_clinic_data_adults.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kLQiOduKVJE",
        "outputId": "e7165916-7be4-4e85-e1d4-0d388b4969ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-12 14:29:59--  https://raw.githubusercontent.com/githubgurus/Health_Data/main/mayo_clinic_data_adults.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45688 (45K) [text/plain]\n",
            "Saving to: ‘mayo_clinic_data_adults.txt’\n",
            "\n",
            "\r          mayo_clin   0%[                    ]       0  --.-KB/s               \rmayo_clinic_data_ad 100%[===================>]  44.62K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-10-12 14:29:59 (2.69 MB/s) - ‘mayo_clinic_data_adults.txt’ saved [45688/45688]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/githubgurus/Health_Data/main/data.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzs-Y-cHMNUf",
        "outputId": "50c921b1-8983-45d8-b8ac-f9de57021630"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-12 14:29:59--  https://raw.githubusercontent.com/githubgurus/Health_Data/main/data.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52427 (51K) [text/plain]\n",
            "Saving to: ‘data.json’\n",
            "\n",
            "\rdata.json             0%[                    ]       0  --.-KB/s               \rdata.json           100%[===================>]  51.20K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-10-12 14:29:59 (2.94 MB/s) - ‘data.json’ saved [52427/52427]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"harvard_health_data.txt\", \"r\") as file1, open(\"mayo_clinic_data_adults.txt\", \"r\") as file2:\n",
        "    data1 = file1.read()\n",
        "    data2 = file2.read()\n",
        "\n",
        "combined_data = data1 + data2\n",
        "\n",
        "with open(\"combined_data.txt\", \"w\") as combined_file:\n",
        "    combined_file.write(combined_data)"
      ],
      "metadata": {
        "id": "AxnrZ-UXKWqW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters, numbers, and extra whitespaces\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "with open(\"combined_data.txt\", \"r\") as combined_file:\n",
        "    data = combined_file.read()\n",
        "\n",
        "preprocessed_data = preprocess_text(data)\n",
        "\n",
        "with open(\"data.txt\", \"w\") as data_file:\n",
        "    data_file.write(preprocessed_data)"
      ],
      "metadata": {
        "id": "zGccpQacKaV2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Load data from data.json\n",
        "with open(\"data.json\", \"r\") as json_file:\n",
        "    data_json = json.load(json_file)\n",
        "\n",
        "# Process data for training\n",
        "with open(\"data.txt\", \"r\") as text_file:\n",
        "    data_text = text_file.read().split('\\n')\n",
        "\n",
        "# Split data into input (symptoms) and output (diseases) lists\n",
        "symptoms = list(data_json[\"symptoms\"].keys())\n",
        "diseases = list(data_json[\"symptoms\"].values())\n",
        "\n",
        "# Tokenize input data (symptoms) for the first model\n",
        "tokenizer1 = Tokenizer()\n",
        "tokenizer1.fit_on_texts(symptoms)\n",
        "symptoms_sequences1 = tokenizer1.texts_to_sequences(symptoms)\n",
        "\n",
        "# Create sequences for output data (diseases) for the first model\n",
        "diseases_sequences1 = []\n",
        "for disease_list in diseases:\n",
        "    for disease in disease_list:\n",
        "        disease_sequences = tokenizer1.texts_to_sequences([disease])\n",
        "        diseases_sequences1.append(disease_sequences)\n",
        "\n",
        "# Create binary labels for the first model\n",
        "num_symptoms = len(symptoms)\n",
        "num_samples = len(symptoms)\n",
        "binary_labels1 = np.zeros((num_samples, num_symptoms), dtype=int)\n",
        "\n",
        "for i, symptom_sequence in enumerate(symptoms_sequences1):\n",
        "    for j in symptom_sequence:\n",
        "        binary_labels1[i, j - 1] = 1\n",
        "\n",
        "# Pad sequences to ensure they have the same length for the first model\n",
        "max_sequence_length1 = max(len(sequence) for sequence in symptoms_sequences1)\n",
        "padded_symptoms1 = pad_sequences(symptoms_sequences1, maxlen=max_sequence_length1, padding='post')\n",
        "\n",
        "# Define and train the first model\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(input_dim=len(symptoms) + 1, output_dim=64, input_length=max_sequence_length1))\n",
        "model1.add(LSTM(64))\n",
        "model1.add(Dense(num_symptoms, activation='sigmoid'))  # Use sigmoid for multi-label classification\n",
        "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model1.fit(padded_symptoms1, binary_labels1, epochs=10)\n",
        "\n",
        "# Save the first model\n",
        "model1.save(\"model1.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMmiu04ORlO4",
        "outputId": "3babbbb9-811d-4998-bff8-8e5cff82e14b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "15/15 [==============================] - 3s 10ms/step - loss: 0.6824 - accuracy: 0.0084\n",
            "Epoch 2/10\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.5630 - accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1886 - accuracy: 0.0607\n",
            "Epoch 4/10\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0580 - accuracy: 0.2594\n",
            "Epoch 5/10\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0415 - accuracy: 0.2594\n",
            "Epoch 6/10\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0374 - accuracy: 0.2594\n",
            "Epoch 7/10\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0357 - accuracy: 0.2594\n",
            "Epoch 8/10\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0348 - accuracy: 0.2594\n",
            "Epoch 9/10\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0342 - accuracy: 0.2594\n",
            "Epoch 10/10\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0337 - accuracy: 0.2594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt-2-simple tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLa4btVNXuc6",
        "outputId": "e72d2d15-d353-41f7-b16d-7fb2aab4993a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.23.5)\n",
            "Collecting toposort (from gpt-2-simple)\n",
            "  Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2023.7.22)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24557 sha256=5c7ce323158db5d6da77c421eb8977e97ab2590bc3857238deb50c0d3a5de6fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/6a/fe/10d3223f78d1ac3e4c83bb4c5e2d28dfb1789c2fb4cc7ea8d0\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 toposort-1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gpt_2_simple as gpt2\n",
        "import tensorflow as tf\n",
        "\n",
        "# Download the GPT-2 model (you can choose a different model size)\n",
        "model_name = \"124M\"\n",
        "gpt2.download_gpt2(model_name=model_name)\n",
        "\n",
        "# Fine-tune the model on your text data\n",
        "# Replace 'your_text_data.txt' with your data file\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess, dataset='data.txt', model_name=model_name, steps=1000)\n",
        "\n",
        "# Convert the GPT-2 model to a format that can be saved as a .h5 file\n",
        "# This step requires TensorFlow 2.x\n",
        "from tensorflow import keras\n",
        "\n",
        "model = gpt2.get_model(sess)\n",
        "\n",
        "# Define a simple Keras model to save the weights\n",
        "keras_model = keras.Sequential()\n",
        "keras_model.add(keras.layers.Input(shape=(model.config.n_ctx,)))\n",
        "keras_model.add(keras.layers.Embedding(model.config.vocab_size, model.config.n_embd))\n",
        "keras_model.layers[0].set_weights([model.wte.numpy()])\n",
        "keras_model.compile()\n",
        "\n",
        "# Save the Keras model as .h5\n",
        "keras_model.save(\"gpt2_model.h5\")\n",
        "\n",
        "# Save the GPT-2 model for future use\n",
        "gpt2.save_gpt2(sess, model_name=model_name)\n",
        "\n",
        "# Close the TensorFlow session\n",
        "gpt2.reset_session(sess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWN-T0F1Ryyi",
        "outputId": "d97397e7-4a1a-40f1-db93-1c1c098ea671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 1.96Git/s]                                                     \n",
            "Fetching encoder.json: 1.05Mit [00:00, 2.25Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 2.89Git/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:15, 31.2Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 220Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 3.46Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 3.09Mit/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:17<00:00, 17.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 2846298 tokens\n",
            "Training...\n",
            "[1 | 156.53] loss=3.69 avg=3.69\n",
            "[2 | 296.81] loss=3.53 avg=3.61\n",
            "[3 | 432.91] loss=3.37 avg=3.53\n",
            "[4 | 569.01] loss=3.31 avg=3.47\n",
            "[5 | 709.27] loss=3.22 avg=3.42\n",
            "[6 | 843.56] loss=3.40 avg=3.42\n",
            "[7 | 979.87] loss=3.34 avg=3.41\n",
            "[8 | 1114.89] loss=3.33 avg=3.40\n",
            "[9 | 1250.22] loss=3.12 avg=3.36\n",
            "[10 | 1388.63] loss=3.27 avg=3.35\n",
            "[11 | 1524.69] loss=3.42 avg=3.36\n",
            "[12 | 1663.71] loss=3.21 avg=3.35\n",
            "[13 | 1799.31] loss=3.04 avg=3.32\n",
            "[14 | 1945.62] loss=3.16 avg=3.31\n",
            "[15 | 2078.60] loss=3.08 avg=3.29\n",
            "[16 | 2216.96] loss=3.07 avg=3.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gpt_2_simple as gpt2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load the fine-tuned GPT-2 model\n",
        "gpt2_model_name = \"124M\"  # Replace with the GPT-2 model name you used\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, model_name=gpt2_model_name)\n",
        "\n",
        "# Load the model1.h5 model\n",
        "model1 = keras.models.load_model(\"model1.h5\")\n",
        "\n",
        "def chat_with_gpt2(input_text):\n",
        "    gpt2_response = gpt2.generate(sess, model_name=gpt2_model_name, prefix=input_text)\n",
        "    gpt2_response = gpt2_response.split(input_text)[-1].strip()\n",
        "    return gpt2_response\n",
        "\n",
        "def chat_with_model1(input_text):\n",
        "    # Preprocess user input if needed\n",
        "    model1_response = model1.predict(np.array([input_text]))\n",
        "    return model1_response\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    gpt2_response = chat_with_gpt2(user_input)\n",
        "    model1_response = chat_with_model1(user_input)\n",
        "\n",
        "    print(f\"GPT-2_fine-tuned: {gpt2_response}\")\n",
        "    print(f\"Model1: {model1_response}\")"
      ],
      "metadata": {
        "id": "K8XedR4EKs1r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}